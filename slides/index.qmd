---
title: Introduction aux algorithmes de _boosting_
subtitle: |
  **[On va vous décoiffer le gradient!]{.orange}**
author: |
  [Olivier Meslin](https://github.com/oliviermeslin)
# date: 
slide-number: true
footer: |
  Introduction aux algorithmes de _boosting_
# uncomment for French presentations:
#lang: fr-FR
# for blind readers:
slide-tone: false
# for @olevitt:
chalkboard: # press the B key to toggle chalkboard
  theme: whiteboard
# uncomment to use the multiplex mode:
#multiplex: true
format:
  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)
  onyxia-revealjs:
  #onyxia-dark-revealjs:
    output-file: index.html
    include-in-header:
      - file: tikzjax.html
    controls: true
css: custom.css
from: markdown+emoji
bibliography: references.bib
---

## Plan

- Un peu d'histoire
- Présentation intuitive  des algorithmes de RF et de boosting:
    - Les arbres de décision;
    - Les algorithmes de RF et de boosting avec les figures;
    - Faire rapidement la liste des hyperparamètres, en faisant des groupes cohérents (arbre unitaire, ensemble, training);
- Comment entraîne-t-on un modèle de ML?



- Les sujets avancés:
    - Changer la fonction de perte;
    - Changer la métrique d'évaluation;

- Illustration
- 
- Faire Rapidement la liste des hyperparamètres, en faisant des groupes cohérents (arbre unitaire, ensemble, training);
- 

- 
- Les hyperparamètres

- Les sujets avancés


# Introduction

```{r, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}
library(magrittr)
list.files(path = "/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/", full.names = TRUE,
           pattern = "*.pdf") %>%
  lapply(function(x, density) {
    # print(x)
    pdftools::pdf_convert(
      x,
      format = "png",
      pages = NULL,
      filenames = stringr::str_replace(x, "-1.pdf", ".png"),
      dpi = density,
      antialias = TRUE,
      opw = "",
      upw = "",
      verbose = TRUE
    )
  },
    density = 500
  )
```

## Un peu d'histoire

<!-- Thoughts on Hypothesis Boosting, https://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf -->
<!-- Robert E. Schapire, The Strength of Weak Learnability -->
<!-- https://maxhalford.github.io/blog/lightgbm-focal-loss/ -->
<!-- https://web.njit.edu/~usman/courses/cs675_fall16/BoostedTree.pdf -->
<!-- https://neptune.ai/blog/lightgbm-parameters-guide -->
<!-- https://rajatmshukla.wordpress.com/2023/02/01/learning-about-gradient-boosting-using-xgboost/ -->
<!-- https://www.linkedin.com/posts/damienbenveniste_machinelearning-datascience-artificialintelligence-activity-7026952338951016448-KtxP?utm_source=share&utm_medium=member_android -->

- Parler des résultats sur les _weak learners_ et _strong learners_;
- Parler de la GBM
- AdaBoost
- XGBoost, LightGBM, CatBoost



## La notion de bonnes pratiques

- [**Origine**]{.blue2} : communauté des développeurs logiciels

::: {.incremental}
- [**Constats**]{.blue2} :
    - le [_"code est plus souvent lu qu'écrit"_]{.green2} ([Guido Van Rossum](https://fr.wikipedia.org/wiki/Guido_van_Rossum))
    - la maintenance d'un code est très coûteuse
:::

. . .

- [**Conséquence**]{.blue2} : un ensemble de [**règles informelles**]{.orange},
conventionnellement acceptées comme produisant des logiciels [**fiables**]{.orange}, [**évolutifs**]{.orange} et [**maintenables**]{.orange}




## Pleins d'équations en vrac

$$\hat{y}_{i} =  \sum_{k=1}^{K} f_k(x_i), f_k \epsilon \mathcal{F}$$
$\hat{y}_{i} =  \sum_{k=1}^{K} f_k(x_i), f_k \epsilon \mathcal{F}$

$\displaystyle \hat{y}_{i} =  \sum_{k=1}^{K} f_k(x_i), f_k \epsilon \mathcal{F}$



$obj(\theta) = \sum_{i}^{n} l(y_{i}, \hat{y}_{i}) + \sum_{k=1}^K \Omega(f_{k})$

$\hat{y}_i^{(0)} =0\\ \hat{y}_i^{(1)} =  f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\ ....\\ \hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)$


## Encore des équations

$$\begin{aligned}obj^{(t)}  &= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\           &= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(x_i)) + \Omega(f_t) + constant\end{aligned}$$

$obj^{(t)}  = \sum_{i=1}^n (y_{i} - (\hat{y}_{i}^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\           = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant$

$obj^{(t)} = \sum_{i=1}^{n} [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t) + constant$

## Toujours des équations

$$g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}}$$
$$h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{{\partial \hat{y}_i^{(t-1)}}^2}$$

 
$\sum_{i=1}^n [g_{i} f_{t}(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t)$

$f_t(x) = w_{q(x)}, w \in R^{T}, q:R^d\rightarrow \{1,2,...,T\}$

## Ya encore des équations


$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2
obj^{(t)} \approx \sum_{i=1}^n [g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\ = \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T$

$obj^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T$

$G_j = \sum_{i\in I_j} g_i\\ H_j = \sum_{i\in I_j} h_i$

$w_j^{\ast} = -\frac{G_j}{H_j+\lambda}\\ \text{obj}^{\ast} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$

$Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma$






<!-- <iframe width="560" height="315" src="/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/rf-1.pdf"></iframe> -->


<!-- ![Dudu](/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/rf-1.pdf) -->

## Random forest

![Random forest](production_figures_files/figure-pdf/rf.png)

## Boosting

![Boosting](production_figures_files/figure-pdf/boosting.png)
