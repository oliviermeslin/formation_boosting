---
title: Introduction aux algorithmes de _boosting_
subtitle: |
  **[On va vous décoiffer le gradient!]{.orange}**
author: |
  [Olivier Meslin](https://github.com/oliviermeslin)
# date: 
slide-number: true
footer: |
  Introduction aux algorithmes de _boosting_
# uncomment for French presentations:
#lang: fr-FR
# for blind readers:
slide-tone: false
# for @olevitt:
chalkboard: # press the B key to toggle chalkboard
  theme: whiteboard
# uncomment to use the multiplex mode:
#multiplex: true
format:
  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)
  onyxia-revealjs:
  #onyxia-dark-revealjs:
    output-file: index.html
    include-in-header:
      - file: tikzjax.html
    controls: true
css: custom.css
from: markdown+emoji
bibliography: references.bib
---

## Plan

<!-- Des macros pour Mathjax -->
::: {.hidden}
$$
  \DeclareMathOperator*{\argmin}{arg\,min}
$$
:::

- Un peu d'histoire
- Présentation intuitive  des algorithmes de RF et de boosting:
    - Les arbres de décision;
    - Les algorithmes de RF et de boosting avec les figures;
    - Faire rapidement la liste des hyperparamètres, en faisant des groupes cohérents (arbre unitaire, ensemble, training);
- Comment entraîne-t-on un modèle de ML?



- Les sujets avancés:
    - Changer la fonction de perte;
    - Changer la métrique d'évaluation;

- Illustration
- 
- Faire Rapidement la liste des hyperparamètres, en faisant des groupes cohérents (arbre unitaire, ensemble, training);
- 

- 
- Les hyperparamètres

- Les sujets avancés


# Introduction

```{r, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}
library(magrittr)
list.files(path = "/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/", full.names = TRUE,
           pattern = "*.pdf") %>%
  lapply(function(x, density) {
    # print(x)
    pdftools::pdf_convert(
      x,
      format = "png",
      pages = NULL,
      filenames = stringr::str_replace(x, "-1.pdf", ".png"),
      dpi = density,
      antialias = TRUE,
      opw = "",
      upw = "",
      verbose = TRUE
    )
  },
    density = 500
  )
```

## Un peu d'histoire

<!-- Thoughts on Hypothesis Boosting, https://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf -->
<!-- Robert E. Schapire, The Strength of Weak Learnability -->
<!-- https://maxhalford.github.io/blog/lightgbm-focal-loss/ -->
<!-- https://web.njit.edu/~usman/courses/cs675_fall16/BoostedTree.pdf -->
<!-- https://neptune.ai/blog/lightgbm-parameters-guide -->


## Des éléments sur `lightGBM`

- une présentation intéressante du GB: https://jmarkhou.com/lgbqr/
- Comment lightGBM choisit le poids de chaque feuille terminale: https://stackoverflow.com/questions/60137453/how-does-lightgbm-or-other-boosted-trees-implementations-with-2nd-order-approxi

## Plan

- Présenter les arbres de régression et de classification. Les notions importantes: splits, noeuds terminaux, partition des données, valeurs, prédictions, profondeur, nombre de feuilles.
- Parler rapidement des résultats sur les _weak learners_ et _strong learners_; Shapire 1990. Principe du _hypothesis boosting_. - AdaBoost est un exemple de boosting qui n'est pas du _gradient boosting_.
- Parler de la GBM, Friedman 2001. _Forward stagewise additive modelling_ (modélisation additive itérative). Centrer la présentation sur l'idée du _gradient boosting_.
- Dire que plusieurs implémentations des approches de _gradient boosting_ sont envisageables: XGBoost, LightGBM, CatBoost.

- Les composantes des modèles de _gradient boosting_:
    - Sur le modèle additif lui-même: nombre d'arbres et _learning rate_;
    - Sur la construction des arbres eux-mêmes: 
        - Paramètres de l'arbre: nombre de feuilles et profondeur, `min_child_weight`;
        - Mode de recherche des splits: `tree_method` (hist, exact, approx), `max_bins`:
        - Données utilisées pour la recherche des splits: _subsample_;
        - Régularisation (signification de chaque hyperparamètre: lambda, alpha, gamma, gain minimum).
    - Sur les fonctions de perte:
        - Expliquer qu'il faut fournir simplement l(), g() et h().
        - Donner des exemples: perte plus exponentielle que la perte quadratique.
        - Expliquer que le modèle final est indépendant de la fonction de perte utilisée pour entraîner le modèle.

- Extensions:
    - Raffiner les fonctions de perte;
    - Traiter les variables catégorielles;
    - Les pondérations: `scale_pos_weight` et `sample_weight`;
    - Utiliser un _warm start_: `base_margin`.

## La notion de bonnes pratiques

- [**Origine**]{.blue2} : communauté des développeurs logiciels

::: {.incremental}
- [**Constats**]{.blue2} :
    - le [_"code est plus souvent lu qu'écrit"_]{.green2} ([Guido Van Rossum](https://fr.wikipedia.org/wiki/Guido_van_Rossum))
    - la maintenance d'un code est très coûteuse
:::

. . .

- [**Conséquence**]{.blue2} : un ensemble de [**règles informelles**]{.orange},
conventionnellement acceptées comme produisant des logiciels [**fiables**]{.orange}, [**évolutifs**]{.orange} et [**maintenables**]{.orange}

# Généralités sur le _machine learning_

## Objectif

L'objectif général des algorithmes de _machine learning_ est d'[__approximer une fonction $\hat{F}$__]{.orange} qui prédit une variable $y$ à partir de variables $\mathbf{x}$: $$\hat{y} = \hat{F} \left(\mathbf{x} \right)$$

. . .

Les algorithmes peuvent avoir des objectifs différents:

::: {.small80}
:::: {.incremental}

- [__Régression__]{.blue2}: Prédire une variable __continue__. <br/>
Ex: prédire le prix d'un logement à partir de ses caractéristiques.
    
- [__Classification binaire__]{.blue2}: Prédire une variable __dichotomique__. <br/>
Ex: prédire si un individu est présent sur le territoire national.
    
- [__Classification multiclasse__]{.blue2}: Prédire une variable __dans un ensemble fini__. <br/>
Ex: prédire la catégorie NAF d'un emploi à partir de sa description.
   
::::
:::

## Quelques notations

::: {.small80}

| Notation   | Définition                           | Machine learning | Économétrie            |
|:--:|------|----|----|
| $y$        | Ce qu'on veut modéliser              | _Target_, _outcome_| Variable dépendante |
|$\mathbf{x}$| Les variables utilisées pour prédire | _Features_         | Variables indépendantes| 
| $\left(y_i, \mathbf{x_i} \right)_{i= 1,\dots,n}$ | Les données dont on dispose | _Instances_ | Observations | 
| $w$ | Les paramètres du modèle | _Weights_| Paramètres ($\beta$)|
|| Procédure de calcul des paramètres du modèle | _Training_ | Estimation |

: {tbl-colwidths="[15,50,35,30]"}


:::

## Fonction de perte

La solution

$$\hat{F} = \argmin_F \mathbb{E}_{y,\mathbf{x}}\left[ L\left(y, F\left(\mathbf{x}\right)\right)\right] $$

. . .

Exemple: On modélise une variable continue avec:

- un modèle linéaire: [$F\left(\mathbf{x}\right) = \mathbf{x} \beta$]{.small80}; 
- une fonction de perte quadratique: [$L\left(y, F\left(\mathbf{x}\right)\right) =  \left(y - F\left(\mathbf{x}\right)\right)^2$]{.small80}. 

La solution $\hat{F}$ est caractérisée par $\beta$ tel que:

$$\hat{\beta} = \argmin_\beta \sum_{i} \left(y_i - \mathbf{x_i}\beta\right)^2$$

. . .

__Avec la perte quadratique et un modèle linéaire, on retombe sur les OLS!__


## Entraînement

Définition de l'entraînement.

Train/test/validation


## _Gradient boosted regression trees_ (GBRT)



Beaucoup d'algorithmes

optimiser sur l'espace des paramètres.



## Pleins d'équations en vrac

$$\hat{y}_{i} =  \sum_{k=1}^{K} f_k(x_i), f_k \epsilon \mathcal{F}$$
$\hat{y}_{i} =  \sum_{k=1}^{K} f_k(x_i), f_k \epsilon \mathcal{F}$

$\displaystyle \hat{y}_{i} =  \sum_{k=1}^{K} f_k(x_i), f_k \epsilon \mathcal{F}$



$obj(\theta) = \sum_{i}^{n} l(y_{i}, \hat{y}_{i}) + \sum_{k=1}^K \Omega(f_{k})$

$\hat{y}_i^{(0)} =0\\ \hat{y}_i^{(1)} =  f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\ ....\\ \hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)$


## Encore des équations

$$\begin{aligned}obj^{(t)}  &= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\           &= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(x_i)) + \Omega(f_t) + constant\end{aligned}$$

$obj^{(t)}  = \sum_{i=1}^n (y_{i} - (\hat{y}_{i}^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\           = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant$

$obj^{(t)} = \sum_{i=1}^{n} [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t) + constant$

## Toujours des équations

$$g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}}$$
$$h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{{\partial \hat{y}_i^{(t-1)}}^2}$$

 
$\sum_{i=1}^n [g_{i} f_{t}(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t)$

$f_t(x) = w_{q(x)}, w \in R^{T}, q:R^d\rightarrow \{1,2,...,T\}$

## Ya encore des équations


$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2
obj^{(t)} \approx \sum_{i=1}^n [g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\ = \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T$

$obj^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T$

$G_j = \sum_{i\in I_j} g_i\\ H_j = \sum_{i\in I_j} h_i$

$w_j^{\ast} = -\frac{G_j}{H_j+\lambda}\\ \text{obj}^{\ast} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$

$Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma$




<!-- <iframe width="560" height="315" src="/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/rf-1.pdf"></iframe> -->


<!-- ![Dudu](/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/rf-1.pdf) -->

## Random forest

![Random forest](production_figures_files/figure-pdf/rf.png)

## Boosting

![Boosting](production_figures_files/figure-pdf/boosting.png)
