---
title: Introduction aux algorithmes de _boosting_
subtitle: |
  **[On va vous décoiffer le gradient!]{.orange}**
author: |
  [Olivier Meslin](https://github.com/oliviermeslin)
# date: 
slide-number: true
footer: |
  Introduction aux algorithmes de _boosting_
# uncomment for French presentations:
#lang: fr-FR
# for blind readers:
slide-tone: false
# for @olevitt:
chalkboard: # press the B key to toggle chalkboard
  theme: whiteboard
# uncomment to use the multiplex mode:
#multiplex: true
format:
  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)
  onyxia-revealjs:
  #onyxia-dark-revealjs:
    output-file: index.html
    include-in-header:
      - file: tikzjax.html
    controls: true
css: custom.css
from: markdown+emoji
bibliography: references.bib
nocite: |
  @*
editor: 
  markdown: 
    wrap: 72
---

## Plan

<!-- Des macros pour Mathjax -->

::: hidden
$$
  \DeclareMathOperator*{\argmin}{arg\,min}
\makeatletter
\let\ams@underbrace=\underbrace
\def\underbrace#1_#2{%
  \setbox0=\hbox{$\displaystyle#1$}%
  \ams@underbrace{#1}_{\parbox[t]{\the\wd0}{#2}}%
}
\makeatother
$$
:::

-   Un peu d'histoire

-   Présentation intuitive des algorithmes de RF et de boosting:

    -   Les arbres de décision;
    -   Les algorithmes de RF et de boosting avec les figures;
    -   Faire rapidement la liste des hyperparamètres, en faisant des
        groupes cohérents (arbre unitaire, ensemble, training);

-   Comment entraîne-t-on un modèle de ML?

-   Les sujets avancés:

    -   Changer la fonction de perte;
    -   Changer la métrique d'évaluation;

-   Illustration

-   

-   Faire Rapidement la liste des hyperparamètres, en faisant des
    groupes cohérents (arbre unitaire, ensemble, training);

-   

-   

-   Les hyperparamètres

-   Les sujets avancés

# Introduction

```{r, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}
library(magrittr)
list.files(path = "/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/", full.names = TRUE,
           pattern = "*.pdf") %>%
  lapply(function(x, density) {
    # print(x)
    pdftools::pdf_convert(
      x,
      format = "png",
      pages = NULL,
      filenames = stringr::str_replace(x, "-1.pdf", ".png"),
      dpi = density,
      antialias = TRUE,
      opw = "",
      upw = "",
      verbose = TRUE
    )
  },
    density = 500
  )
```

## Un peu d'histoire

<!-- Thoughts on Hypothesis Boosting, https://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf -->

<!-- Robert E. Schapire, The Strength of Weak Learnability -->

<!-- https://maxhalford.github.io/blog/lightgbm-focal-loss/ -->

<!-- https://web.njit.edu/~usman/courses/cs675_fall16/BoostedTree.pdf -->

<!-- https://neptune.ai/blog/lightgbm-parameters-guide -->

## Des éléments sur `lightGBM`

-   une présentation intéressante du GB: https://jmarkhou.com/lgbqr/
-   Comment lightGBM choisit le poids de chaque feuille terminale:
    https://stackoverflow.com/questions/60137453/how-does-lightgbm-or-other-boosted-trees-implementations-with-2nd-order-approxi

## Objectifs de la formation

- Trois contributions théoriques à comprendre:
    - Qu'est-ce qu'une approche de _boosting_?
    - Qu'est-ce que le _gradient boosting_?
    - Comment construit-on les arbres de régression?
- ASPECTS PRATIQUES A COMPLETER.


## Plan

-   Présenter les arbres de régression et de classification. Les notions
    importantes: splits, noeuds terminaux, partition des données,
    valeurs, prédictions, profondeur, nombre de feuilles.

-   Parler rapidement des résultats sur les *weak learners* et *strong
    learners*; Shapire 1990. Principe du *hypothesis boosting*. -
    AdaBoost est un exemple de boosting qui n'est pas du *gradient
    boosting*.

-   Parler de la GBM, Friedman 2001. *Forward stagewise additive
    modelling* (modélisation additive itérative). Centrer la
    présentation sur l'idée du *gradient boosting*.

-   Dire que plusieurs implémentations des approches de *gradient
    boosting* sont envisageables: XGBoost, LightGBM, CatBoost.

-   Les composantes des modèles de *gradient boosting*:

    -   Sur le modèle additif lui-même: nombre d'arbres et *learning
        rate*;
    -   Sur la construction des arbres eux-mêmes:
        -   Paramètres de l'arbre: nombre de feuilles et profondeur,
            `min_child_weight`;
        -   Mode de recherche des splits: `tree_method` (hist, exact,
            approx), `max_bins`:
        -   Données utilisées pour la recherche des splits: *subsample*;
        -   Régularisation (signification de chaque hyperparamètre:
            lambda, alpha, gamma, gain minimum).
    -   Sur les fonctions de perte:
        -   Expliquer qu'il faut fournir simplement l(), g() et h().
        -   Donner des exemples: perte plus exponentielle que la perte
            quadratique.
        -   Expliquer que le modèle final est indépendant de la fonction
            de perte utilisée pour entraîner le modèle.

-   Extensions:

    -   Raffiner les fonctions de perte;
    -   Traiter les variables catégorielles;
    -   Les pondérations: `scale_pos_weight` et `sample_weight`;
    -   Utiliser un *warm start*: `base_margin`.

## La notion de bonnes pratiques

-   [**Origine**]{.blue2} : communauté des développeurs logiciels

::: incremental
-   [**Constats**]{.blue2} :
    -   le [*"code est plus souvent lu qu'écrit"*]{.green2} ([Guido Van
        Rossum](https://fr.wikipedia.org/wiki/Guido_van_Rossum))
    -   la maintenance d'un code est très coûteuse
:::

. . .

-   [**Conséquence**]{.blue2} : un ensemble de [**règles
    informelles**]{.orange}, conventionnellement acceptées comme
    produisant des logiciels [**fiables**]{.orange},
    [**évolutifs**]{.orange} et [**maintenables**]{.orange}


## L'avenir c'est maintenant


![](Tweet_Bojan_Tunguz.png)


# Généralités sur l'apprentissage supervisé

## Introduction

L'objectif général des algorithmes d'apprentissage supervisé
(*supervised learning*) est d'[**approximer une fonction**]{.orange} $F$
qui prédit une variable $y$ à partir d'un ensemble de variables
$\mathbf{x}$. On obtient une fonction $\hat{F}$:

$$\hat{y} = \hat{F} \left(\mathbf{x} \right)$$

. . .

Les algorithmes peuvent avoir des objectifs différents:

::: small80
::: incremental
-   [**Régression**]{.blue2}: Prédire une variable **continue**. <br/>
    Ex: prédire le prix d'un logement à partir de ses caractéristiques.

-   [**Classification binaire**]{.blue2}: Prédire une variable
    **dichotomique**. <br/> Ex: prédire si un individu est présent sur
    le territoire national.

-   [**Classification multiclasse**]{.blue2}: Prédire une variable
    **dans un ensemble fini**. <br/> Ex: prédire la catégorie NAF d'un
    emploi à partir de sa description.
:::
:::

## Trois notions essentielles

::: small90
Une approche d'apprentissage supervisé comprend trois éléments:

-   Un [**modèle**]{.orange} $F$ qui relie $y$ et $\mathbf{x}$ et
    $y = F\left(\mathbf{x}, \mathbf{\theta}\right)$;<br/> Exemple:
    $y = \mathbf{x} \mathbf{\beta}$

-   Des [**paramètres**]{.orange}
    $\mathbf{\theta} = \{w_j\}_{j=1,\dots,d}$ qui doivent être estimés à
    partir des données; <br/> Exemple: $\mathbf{\beta}$ dans
    $y = \mathbf{x} \mathbf{\beta}$

-   Une [**fonction objectif**]{.orange}
    $Obj(\mathbf{\theta}, y, \mathbf{x})$ avec une [__fonction de
    perte__]{.orange} et un [__terme de régularisation__]{.orange}.<br/>
:::

## Fonction objectif

La fonction objectif rassemble deux fonctions:

::: small90
-   La [**fonction de perte**]{.orange} $L$ mesure pour chaque
    observation la distance entre l'observé $y_i$ et la prédiction
    $F\left(\mathbf{x}_i \right)$;
-   La [**fonction de régularisation**]{.orange} $\Omega$ mesure la
    complexité du modèle.
:::

$$Obj\left(\mathbf{\theta}, y, \mathbf{x}\right) = \underbrace{L\left(\mathbf{\theta}, y, \mathbf{x}\right)}_{\substack{\bf\text{Fonction} \\ \bf\text{de perte}}} + \underbrace{\Omega\left(\mathbf{\theta}\right)}_{\substack{\bf\text{Fonction de} \\ \bf\text{régularisation}}}$$

. . .

::: small80
Petits abus de notation: on écrit $F\left(\mathbf{x}\right)$ au lieu de
$F\left(\mathbf{x}, \mathbf{\theta}\right)$,
$Obj\left(\mathbf{\theta}\right)$ au lieu de
$Obj\left(\mathbf{\theta}, y, \mathbf{x}\right)$, et
$L\left(y, \mathbf{x}\right)$ au lieu de
$L\left(\mathbf{\theta}, y, \mathbf{x}\right)$
:::

## Fonction de perte

-   La [**fonction de perte**]{.orange} $L$ mesure pour chaque
    observation la distance entre l'observé $y_i$ et la prédiction
    $F\left(\mathbf{x}_i \right)$;

. . .

-   Elle est habituellement convexe et dérivable deux fois.

. . .

-   Fonctions de perte classiques:
    -   La perte quadratique:
        $L\left(y, F\left(\mathbf{x}\right)\right) = \frac{1}{2}\left(y - F\left(\mathbf{x}\right)\right)^2$
    -   La perte absolue:
        $L\left(y, F\left(\mathbf{x}\right)\right) = \left|y - F\left(\mathbf{x}\right)\right|$
    -   La perte logloss:
        $L\left(y, F\left(\mathbf{x}\right)\right) = - \left(y \log \left(p\left(\mathbf{x}\right)\right) + \left(1 - y \right) \log \left(1 - p\left(\mathbf{x}\right)\right)\right)$
-   Il est possible d'utiliser une fonction de perte plus originale.

## Fonction de régularisation

-   La [**fonction de régularisation**]{.orange} $\Omega$ mesure la
    complexité du modèle. Elle est strictement croissante avec des
    mesures de cette complexité (nombre de paramètres, valeur absolue
    des paramètres...).

. . .

-   Fonctions de régularisation classiques:
    -   Aucune régularisation: $\Omega\left(\mathbf{w}\right) = 0$
        (fréquent en économétrie!)
    -   La norme L1 (lasso):
        $\Omega\left(\mathbf{w}\right) = \lambda \sum_{j=1}^d \left|w\right|$
    -   La norme L2 (ridge):
        $\Omega\left(\mathbf{w}\right) = \lambda \sum_{j=1}^d w^2$

Les fonctions de régularisation peuvent être spécifiques à un
algorithme.

## Entraînement d'un modèle

La procédure statistique qui utilise la [**fonction objectif**]{.orange}
pour calculer les [**paramètres**]{.orange} du [**modèle**]{.orange} à
partir d'un jeu de données
$\left(y_i, \mathbf{x_i} \right)_{i= 1,\dots,n}$ est appelée
[**entraînement**]{.orange} du modèle.

. . .

À partir d'un jeu de données
$\left(y_i, \mathbf{x_i} \right)_{i= 1,\dots,n}$, on obtient
l'approximation $\hat{F}$ en [minimisant l'espérance
conditionnelle]{.orange} de la fonction objectif $Obj$ par rapport aux
paramètres $\bf\theta$. $\hat{F}$ est caractérisé par $\hat{\bf\theta}$
tel que:

$$\hat{\bf\theta} = \argmin_{\bf\theta} \mathbb{E}_{y,\mathbf{x}}\left[ Obj\left(\mathbf{\theta}, y, \mathbf{x} \right)\right]$$

## Entraînement d'un modèle: exemple

On modélise une variable continue avec:

-   un modèle linéaire:
    $F\left(\mathbf{x}\right) = \mathbf{x} \mathbf{\beta}$;
-   une fonction de perte quadratique:
    $L\left(y, F\left(\mathbf{x}\right)\right) = \frac{1}{2}\left(y - F\left(\mathbf{x}\right)\right)^2$;
-   pas de fonction de régularisation.

La fonction $\hat{F}$ est caractérisée par $\mathbf{\beta}$ tel que:

$$\hat{\mathbf{\beta}} = \argmin_\mathbf{\beta} \sum_{i} \left(y_i - \mathbf{x_i}\mathbf{\beta}\right)^2$$

. . .

**On retombe sur les OLS!**

## La notion d'*overfitting*

A écrire

## Données d'entraînement et données de test

A écrire

## Quelques définitions

::: small60
|                     Notation                     | Définition                                                                 | Machine learning                       | Économétrie                                        |
|:---------------:|----------------------|-----------------|-----------------|
|                       $y$                        | Ce qu'on veut modéliser                                                    | *Target*, *outcome*                    | Variable dépendante                                |
|                   $\mathbf{x}$                   | Les variables utilisées pour prédire                                       | *Features*                             | Variables indépendantes                            |
| $\left(y_i, \mathbf{x_i} \right)_{i= 1,\dots,n}$ | Les données dont on dispose                                                | *Instances*                            | Observations                                       |
|                       $w$                        | Les paramètres du modèle                                                   | *Weights*                              | Paramètres ($\mathbf{\beta}$)                      |
|                                                  | Procédure de calcul des paramètres du modèle                               | *Training*, entraînement               | Estimation                                         |
|                                                  | Le fait que le modèle est *trop* performant sur les données d'entraînement | *Overfitting*, surapprentissage        | Pas de terme [(c'est bien le problème!)]{.small50} |
|                                                  | Données utilisées pour l'entraînement du modèle                            | *Training set*, données d'entraînement |                                                    |
|                                                  | Données utilisées pour l'évaluation du modèle                              | *Test set*, données de test            |                                                    |

: {tbl-colwidths="\[15,50,35,30\]"}
:::

# Qu'est-ce que le *boosting*?

## Estimation d'une fonction inconnue

L'objectif de l'apprentissage supervisé est d'[**approximer une fonction
inconnue**]{.orange} $F: \mathbf{x} \mapsto y$ en minimisant l'espérance
conditionnelle d'une fonction de perte
$L\left(y, F\left(\mathbf{x}\right)\right)$ sur un ensemble
d'entraînement $\left(y_i, \mathbf{x_i} \right)_{i= 1,\dots,n}$.
Formellement:

$$\hat{F} = \argmin_F \mathbb{E}_{y,\mathbf{x}}\left[ L\left(y, F\left(\mathbf{x}\right)\right)\right] $$

. . .

C'est un problème compliqué car on cherche la fonction $F$ parmi
l'ensemble des fonctions possibles!

## Estimation paramétrique {.small90}

::: incremental
-   Principe: rechercher $F\left(\mathbf{x}\right)$ dans un ensemble de
    fonctions paramétrisées par un vecteur fini de paramètres $\theta$.
    <br/> [Exemple: $F\left(\mathbf{x}\right) = \mathbf{x\beta}$ avec
    $\mathbf{\beta} = \{\beta_1, \dots, \beta_d\}$]{.small90}

-   Cette restriction permet de se ramener à un problème d'optimisation
    sur l'espace des paramètres: $\hat{F}$ est caractérisée par ses
    paramètres $\hat{\bf\theta}$ tels que <br/>
    $$\hat{\bf\theta} = \argmin_{\bf\theta} \mathbb{E}_{y,\mathbf{x}}\left[ L\left(y, F\left(\mathbf{x}, \mathbf{\theta}\right)\right)\right]$$

-   Si on veut obtenir un modèle performant, le nombre de paramètres
    peut devenir élevé et l'entraînement du modèle très compliqué...
:::

## Les approches ensemblistes {.small90}

::: incremental
-   Principe: créer un modèle très précis en combinant un grand nombre
    de modèles simples comprenant un nombre restreint de paramètres
    (*base learners* ou *weak learners*).

-   Les méthodes ensemblistes sont diversifiées:

    -   Nature du *base learner* $\Rightarrow$ des
        [**arbres**]{.orange} le plus souvent;
    -   Méthode pour combiner les *base learners* (moyenne, vote
        majoritaire...);
    -   Méthode d'entraînement du modèle.

-   Deux grandes approches ensemblistes:

    -   *Bagging* et *random forest* (@breiman1996bagging,
        @breiman2001random);
    -   *Boosting* (@shapire1990strength, @freund1997decision).
:::

## Les arbres de décision et de classification

-   Présenter les arbres de régression et de classification. Les notions
    importantes: splits, noeuds terminaux, partition des données,
    valeurs, prédictions, profondeur, nombre de feuilles.

Parler des arbres: ESM chapitre 9, chapitre 10 formule page 353

## *Bagging* et *random forests* {.small90}

-   Principe de ces approches:
    -   *Bagging* (@breiman1996bagging): chaque *base learner* est
        entraîné **séparément** sur un échantillon aléatoire
        d'observations;
    -   *Random forest* (@breiman2001random): chaque *base learner* est
        entraîné **séparément** sur un échantillon aléatoire
        d'observations et de variables;

. . .

- Point essentiel: les modèles sont entraînés [__indépendamment__]{.blue2} les uns des autres.

. . .

-   Les prédictions des *base learners* sont combinées pour obtenir la
    prédiction finale. Exemple:
    $$F\left(\mathbf{x}\right) = \frac{1}{M} \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right)$$

## *Random forests*

![Random forest](production_figures_files/figure-pdf/rf.png)

## Le *boosting* {.small90}

Le _boosting_ est une approche ensembliste qui ne traite pas les modèles de base séparément les uns des autres.

. . .

Imaginons qu'on veuille entraîner le modèle suivant:

$$F\left(\mathbf{x}\right) = \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right)$$

. . .

$\hat{F}$ est caractérisée par les paramètres $\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}$ tels que
$$\argmin_{\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}} \sum_{i=1}^N L\left(y_i, \sum_{m=1}^M \beta_m f\left(\mathbf{x}_i, \mathbf{\theta}_m\right)\right)$$

. . .

C'est un problème très compliqué dès que $M$ est élevé!

## Le *boosting* {.small90}

Le *boosting* combine l'[**approche ensembliste**]{.orange} avec une [**modélisation additive par étapes**]{.orange} (*forward stagewise additive modeling*).

. . .

Deux propriétés essentielles:

- La modélisation additive par étapes décompose l'entraînement d'un très gros modèle (procédure très complexe) en une [séquence d'entraînements de petits modèles]{.blue2} (procédure beaucoup plus simple);

. . .

- Chaque _base learner_ essaie de [corriger les erreurs du modèle issu des étapes précédentes]{.blue2}.


## Modélisation additive par étapes {.small90}

[__La modélisation additive par étapes simplifie massivement l'entraînement du modèle__]{.blue2}:

. . .

- On commence par entraîner un seul _base learner_;
- On ajoute au modèle un deuxième _base learner_ sans modifier les paramètres du premier _base learner_ déjà entraîné;
- On entraîne ce deuxième _base learner_ de façon à améliorer le plus possible le modèle global (la somme des deux _base learners_).
- On ajoute un troisième _base learner_ et on l'entraîne...
- ...

## Modélisation additive par étapes

::: {.callout-note appearance="simple"}
## Algorithme de modélisation additive par étapes

1.  Commencer par $f_0\left(\mathbf{x}\right) = 0$.
2.  Pour $m = 1, \dots, M:$
    (a) Entraîner le $m$-ième modèle:
    $$\left(\hat{\beta}_m, \hat{\theta}_m\right) = \argmin_{\beta, \mathbf{\theta}} \sum_{i=1}^N L\left(y_i, f_{m-1}\left(\mathbf{x}_i\right) + \beta b\left(\mathbf{x}_i, \mathbf{\theta}\right)\right)$$
    (b) Définir $f_m\left(\mathbf{x}\right) = f_{m-1}\left(\mathbf{x}\right) + \hat{\beta}_m b\left(\mathbf{x}_i, \mathbf{\hat{\theta}_m}\right)$
:::

## Correction progressive des erreurs {.small90}

- [__À chaque étape, le _base learner_ essaie de corriger les erreurs du modèle issu des étapes précédentes.__]{.blue2}

. . .

- Il est possible de faire le lien avec la notion de résidu dans un cas particulier.

. . .

- Hypothèses: on veut entraîner le modèle $F\left(\mathbf{x}\right) = \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right)$ avec une fonction de perte quadratique $L\left(y, F\left(\mathbf{x}\right)\right) = \frac{1}{2}\left(y - F\left(\mathbf{x}\right)\right)^2$.

. . .

- On sait que $f_{m}\left(\mathbf{x}_i\right) = f_{m-1}\left(\mathbf{x}_i\right) + \beta b\left(\mathbf{x}_i, \mathbf{\gamma}_m\right)$.

## Correction progressive des erreurs {.small90}

::: {.small80}

::: incremental

- On peut réécrire la fonction de perte utilisée pour entraîner le $m$-ième modèle:\begin{align*}
L\bigl(y_i, f_{m}\left(\mathbf{x}_i\right)\bigr) 
  &= \bigl(y_i - f_{m}\left(\mathbf{x}_i\right)\bigr)^2 \\
  &= \biggl(\underbrace{y_i - f_{m-1}\left(\mathbf{x}_i\right)}_{\substack{\text{Résidu à l'issue}\\ \text{de l'étape } m-1}} - \beta b\left(\mathbf{x}_i, \mathbf{\gamma}_m\right)\biggr)^2 \\
  &= \biggl(\hat{r}_{im} - \beta b\left(\mathbf{x}_i, \mathbf{\gamma}_m\right)\biggr)^2
\end{align*}

- L'entraînement du $m$-ième modèle vise à calculer $\hat{\beta}_m, \hat{\gamma}_m$ tels que $$\hat{\beta}_m, \hat{\gamma}_m = \argmin_{\{\beta_m, \mathbf{\gamma}_m\}} \sum_{i=1}^N \biggl(\hat{r}_{im} - \beta_m b\left(\mathbf{x}_i, \mathbf{\gamma}_m\right)\biggr)^2$$

- [Le $m$-ième modèle essaie de prédire les résidus, c'est-à-dire ce que les $m-1$ arbres précédents n'ont pas réussi à prédire!]{.blue2}

:::
:::


## Correction progressive des erreurs {.small90}

![Figure svg sur loosting](production_figures_files/figure-pdf/boosting.png)

## Correction progressive des erreurs {.small90}

Trois intuitions à partir de ce cas particulier:

- Chaque arbre n'est [pas indépendant des arbres précédents]{.blue2};
- Chaque arbre est [ciblé sur les erreurs laissées par les arbres précédents]{.blue2} $\Rightarrow$ cela évite les redites et rend le modèle final performant;
- Cette focalisation sur les erreurs crée un [sérieux risque de surapprentissage de l'ensemble d'entraînement]{.blue2}.

## Résumé sur le _boosting_ {.small90}

-   Une approche ensembliste;<br/> $\Rightarrow$ Beaucoup de petits
    modèles (arbres de régression) plutôt qu'un gros modèle.
-   Une modélisation additive par étapes;<br/> $\Rightarrow$ On entraîne
    les modèles séquentiellement.
-   Chaque modèle essaie de corriger les erreurs du modèle issu des étapes précédentes;<br/> $\Rightarrow$ Modèle final performant mais risque de surapprentissage.

# Qu'est-ce que le _gradient boosting_?

##




## *Gradient boosted regression trees* (GBRT) {.small90}

Je reprends exactement les notations de l'article principal qui décrit XGBoost.

. . .

Quatre étapes:

- Présentation du modèle qu'on veut entraîner;
- Reformulation du modèle pour faire apparaître les gradients;
- Calcul des poids optimaux et de la qualité d'un arbre dont la structure est donnée;
- Définition d'une méthode et de critères permettant de construire un arbre.


## Le modèle qu'on veut entraîner {.small90}

::: {.small80}
:::: {.incremental}

- On veut entraîner un [modèle]{.orange} comprenant $K$ arbres de régression: $$\hat{y}_{i} = \phi\left(\mathbf{x}_i\right) = \sum_{k=1}^{K} f_k\left(\mathbf{x}_i\right), f_k \epsilon \mathcal{F}$$

- Chaque arbre est défini par trois [paramètres]{.orange}: sa structure (fonction $q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$), son nombre de feuilles terminales $T$ et ses poids $\mathbf{w}\in \mathbb{R}^T$.

- On entraîne le modèle avec une [fonction-objectif]{.orange} constituée d'une [fonction de perte]{.orange} dérivable et convexe $l$ et d'une [fonction de régularisation]{.orange} $\Omega$.$$\mathcal{L}(\phi) = \underbrace{\sum_i l(\hat{y}_{i}, y_{i})}_{\substack{\text{Perte sur les} \\ \text{observations}}} + \underbrace{\sum_k \Omega(f_{k})}_{\substack{\text{Fonction de} \\ \text{régularisation}}}\hspace{5mm}\text{avec}\hspace{5mm}\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$

- Notation: $\hat{y}_i^{(t)}$ désigne la prédiction à lissue de l'étape $t$: $\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(\mathbf{x}_i)$.

<!-- $\mathcal{F} = \{f(\mathbf{x}) = w_{q(\mathbf{x})}\}(q:\mathbb{R}^m \rightarrow T,w \in \mathbb{R}^T)$ -->

<!-- $\hat{y}_i^{(0)} = 0\\ \hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\ ....\\ \hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)$ -->

::::
:::

## Faire apparaître le gradient {.small90}

::: {.small80}
:::: {.incremental}

- On écrit la fonction-objectif au moment de l'entraînement du $t$-ième arbre: $$\begin{aligned}\mathcal{L}^{(t)} 
&= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t)}) + \sum_{k=1}^t\Omega(f_k) \\
&= \sum_{i=1}^{n} l\left(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i)\right) + \Omega(f_t) + constant
\end{aligned}$$

- On fait un développement limité d'ordre 2 de $l(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i))$ au voisinage de $f_{t}(\mathbf{x}_i)$: $$\mathcal{L}^{(t)} \approx \sum_{i=1}^{n} [l(y_i, \hat{y}_{i}^{(t-1)}) + g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \Omega(f_t)$$ avec $$g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}} \hspace{4mm}\text{et}\hspace{4mm} h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{{\partial \hat{y}_i^{(t-1)}}^2}$$



::::
:::

## Faire apparaître les poids $w_j$ {.small90}

::: {.small80}
:::: {.incremental}

- On peut simplifier $\mathcal{\tilde{L}}^{(t)}$ en enlevant les termes $l(y_i, \hat{y}_{i}^{(t-1)})$ car ils sont constants et en remplaçant $\Omega(f)$ par sa définition: 
$$\mathcal{\tilde{L}}^{(t)} = \sum_{i=1}^{n} [g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$

- On définit $I_j = \{ i | q(\mathbf{x}_i) = j \}$ l'ensemble des observations situées sur la feuille $j$ et on réorganise $\mathcal{\tilde{L}}^{(t)}$ pour faire apparaître les poids $w_i$ du $t$-ième arbre:
$$\begin{alignat*}{4}
\mathcal{\tilde{L}}^{(t)}
  &= \sum_{j=1}^{T} \sum_{i\in I_j} \Bigl[g_i f_t(\mathbf{x}_i) &&+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)\Bigr] &&+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
  &= \sum_{j=1}^{T} \sum_{i\in I_j} \Bigl[g_i w_j &&+ \frac{1}{2} h_i w_j^2\Bigl] &&+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
  &= \sum^T_{j=1} \Bigl[w_j(\sum_{i\in I_j} g_i) &&+ \frac{1}{2} w_j^2 (\sum_{i\in I_j} h_i + \lambda) \Bigr] &&+ \gamma T
\end{alignat*}$$

:::
::::

## Calculer les poids optimaux $w_j$ {.small90}

::: {.small90}
:::: {.incremental}

- Pour une structure d'arbre donnée ($q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$), on peut facilement calculer les poids optimaux et la valeur optimale de la fonction-objectif:
$$w_j^{\ast} = -\frac{\sum_{i\in I_j} g_i}{\sum_{i\in I_j} h_i+\lambda}$$
$$\mathcal{\tilde{L}}^{(t)}(q) = -\frac{1}{2} \sum_{j=1}^T \frac{\left(\sum_{i\in I_j} g_i\right)^2}{\sum_{i\in I_j} h_i+\lambda} + \gamma T$$

- [__Ces deux formules sont très utiles!__]{.blue2}

:::
::::

## Évaluer la qualité d'un arbre et d'un _split_ {.small90}

::: {.small80}
:::: {.incremental}

- On peut utiliser la formule donnant la valeur optimale de la fonction-objectif pour comparer la qualité de deux arbres.

- On également peut utiliser cette formule pour évaluer le gain (réduction de la fonction-objectif) induit par un _split_ marginal.

- Imaginons qu'on décompose la feuille $I$ en deux nouvelles feuilles $I_L$ et $I_R$ (avec $I = I_L \cup I_R$). Le gain potentiel par ce _split_ est:
$$\mathcal{L}_{\text{split}} = \frac{1}{2} \left[\frac{\left(\sum_{i\in I_L} g_i\right)^2}{\sum_{i\in I_L} h_i+\lambda}+\frac{\left(\sum_{i\in I_R} g_i\right)^2}{\sum_{i\in I_R} h_i+\lambda}-\frac{\left(\sum_{i\in I} g_i\right)^2}{\sum_{i\in I} h_i+\lambda}\right] - \gamma$$

- Cette dernière formule est essentielle: elle permet de comparer les _splits_ possibles lorsqu'on construit un arbre.

:::
::::

## Comment construire un arbre

- Bien dire qu'on est en approche _greedy_ (glouton/morfal/bourrin).
- Décrire les différentes façons de rechercher les _splits_.
- Parler à la fin du _pruning_.



## Encore des équations


$obj^{(t)} = \sum_{i=1}^n (y_{i} - (\hat{y}_{i}^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\ = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant$

$obj^{(t)} = \sum_{i=1}^{n} [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t) + constant$

## Toujours des équations



$\sum_{i=1}^n [g_{i} f_{t}(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t)$

$f_t(x) = w_{q(x)}, w \in R^{T}, q:R^d\rightarrow \{1,2,...,T\}$

## Ya encore des équations

$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 obj^{(t)} \approx \sum_{i=1}^n [g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\ = \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T$

$obj^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T$

$G_j = \sum_{i\in I_j} g_i\\ H_j = \sum_{i\in I_j} h_i$

$w_j^{\ast} = -\frac{G_j}{H_j+\lambda}\\ \text{obj}^{\ast} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$

$Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma$

<!-- <iframe width="560" height="315" src="/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/rf-1.pdf"></iframe> -->

<!-- ![Dudu](/home/onyxia/work/formation_boosting/slides/production_figures_files/figure-pdf/rf-1.pdf) -->

## Random forest

![Random forest](production_figures_files/figure-pdf/rf.png)

## Boosting

![Boosting](production_figures_files/figure-pdf/boosting.png)

## Références

::: small80
::: {#refs}
:::
:::
